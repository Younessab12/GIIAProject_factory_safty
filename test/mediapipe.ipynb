{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import cvzone\n",
    "import math\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import scipy.fftpack as fftpack\n",
    "\n",
    "\n",
    "caminput =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##mediapipe hand detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'landmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Amine\\Desktop\\AI\\GIIAProject_factory_safty\\test\\mediapipe.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Amine/Desktop/AI/GIIAProject_factory_safty/test/mediapipe.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m img\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Amine/Desktop/AI/GIIAProject_factory_safty/test/mediapipe.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(img, cv2\u001b[39m.\u001b[39mCOLOR_RGB2BGR)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Amine/Desktop/AI/GIIAProject_factory_safty/test/mediapipe.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, landmark \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(res\u001b[39m.\u001b[39;49mface_landmarks\u001b[39m.\u001b[39;49mlandmark):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Amine/Desktop/AI/GIIAProject_factory_safty/test/mediapipe.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m             \u001b[39m# Convert normalized coordinates to pixel values\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Amine/Desktop/AI/GIIAProject_factory_safty/test/mediapipe.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m             x \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(landmark\u001b[39m.\u001b[39mx \u001b[39m*\u001b[39m width)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Amine/Desktop/AI/GIIAProject_factory_safty/test/mediapipe.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m             y \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(landmark\u001b[39m.\u001b[39my \u001b[39m*\u001b[39m height)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'landmark'"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(caminput)\n",
    "StartCoords = []\n",
    "pose = mp.solutions.pose.Pose(model_complexity=2,\n",
    "    enable_segmentation=True,\n",
    "    min_detection_confidence=0.5)\n",
    "hand = mp.solutions.hands.Hands(max_num_hands=2,\n",
    "    min_detection_confidence=0.5)\n",
    "prev_time = 0\n",
    "new_time = 0\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.flip(img, 1)\n",
    "        img.flags.writeable = False\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "\n",
    "        res = holistic.process(img)\n",
    "        pose_res = pose.process(img)\n",
    "        hand_res = hand.process(img)\n",
    "        img.flags.writeable = True\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        for idx, landmark in enumerate(res.face_landmarks.landmark):\n",
    "                    # Convert normalized coordinates to pixel values\n",
    "                    x = int(landmark.x * width)\n",
    "                    y = int(landmark.y * height)\n",
    "\n",
    "                    # Draw a small circle at each landmark.\n",
    "                    cv2.circle(img, (x, y), 1, (0, 255, 0), -1)\n",
    "\n",
    "                    # Write the index number next to the landmark\n",
    "                    cv2.putText(img, str(idx), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "\n",
    "        # if res.face_landmarks:\n",
    "        #   face_landmarks = res.face_landmarks\n",
    "        #   top_left_index = 116  # Example index for top-left corner\n",
    "        #   top_right_index = 101  # Example index for top-right corner\n",
    "        #   bottom_left_index = 192  # Example index for bottom-left corner\n",
    "        #   bottom_right_index = 206  # Example index for bottom-right corner\n",
    "\n",
    "        #   # Extracting coordinates\n",
    "        #   top_left = face_landmarks.landmark[top_left_index]\n",
    "        #   top_right = face_landmarks.landmark[top_right_index]\n",
    "        #   bottom_left = face_landmarks.landmark[bottom_left_index]\n",
    "        #   bottom_right = face_landmarks.landmark[bottom_right_index]\n",
    "\n",
    "        #   # Convert normalized coordinates to pixel values\n",
    "        #   points = [(int(pt.x * width), int(pt.y * height)) for pt in [top_left, top_right, bottom_left, bottom_right]]\n",
    "\n",
    "        #   left_cheek = img[\n",
    "        #       points[0][1]:points[3][1],\n",
    "        #       points[0][0]:points[3][0]\n",
    "        #   ]\n",
    "\n",
    "        #   cv2.imshow(\"l\", left_cheek)\n",
    "\n",
    "        #   # Draw rectangle\n",
    "        #   cv2.rectangle(img, points[0], points[3], (0, 255, 0), 2)\n",
    "\n",
    "        # if res.face_landmarks:\n",
    "        #   face_landmarks = res.face_landmarks\n",
    "        #   top_left_index = 348  # Example index for top-left corner\n",
    "        #   top_right_index = 101  # Example index for top-right corner\n",
    "        #   bottom_left_index = 192  # Example index for bottom-left corner\n",
    "        #   bottom_right_index = 433  # Example index for bottom-right corner\n",
    "\n",
    "        #   # Extracting coordinates\n",
    "        #   top_left = face_landmarks.landmark[top_left_index]\n",
    "        #   top_right = face_landmarks.landmark[top_right_index]\n",
    "        #   bottom_left = face_landmarks.landmark[bottom_left_index]\n",
    "        #   bottom_right = face_landmarks.landmark[bottom_right_index]\n",
    "\n",
    "        #   # Convert normalized coordinates to pixel values\n",
    "        #   points = [(int(pt.x * width), int(pt.y * height)) for pt in [top_left, top_right, bottom_left, bottom_right]]\n",
    "\n",
    "        #   points = [(int(pt.x * width), int(pt.y * height)) for pt in [top_left, top_right, bottom_left, bottom_right]]\n",
    "\n",
    "        #   right_cheek = img[\n",
    "        #       points[0][1]:points[3][1],\n",
    "        #       points[0][0]:points[3][0]\n",
    "        #   ]\n",
    "\n",
    "        #   cv2.imshow(\"r\", right_cheek)\n",
    "\n",
    "        #   # Draw rectangle\n",
    "        #   cv2.rectangle(img, points[0], points[3], (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Hand Tracking\", img)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "cap = cv2.VideoCapture(0)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "vid_data=[]\n",
    "curr_time=time.time()\n",
    "while cap.isOpened():\n",
    "    ret,frame=cap.read()\n",
    "    if time.time()-curr_time>10:\n",
    "        break\n",
    "    frame=cv2.flip(frame,1)\n",
    "    if ret is True:\n",
    "        cv2.imshow('frame',frame)\n",
    "        vid_data.append(frame)\n",
    "        k=cv2.waitKey(10)\n",
    "        if k ==ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "vid_data=np.array(vid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build gaussian pyramid for video\n",
    "def gaussian_video(video_tensor,levels=3):\n",
    "    for i in range(0,video_tensor.shape[0]):\n",
    "        frame=video_tensor[i]\n",
    "        pyr=build_gaussian_pyramid(frame,level=levels)\n",
    "        gaussian_frame=pyr[-1]\n",
    "        if i==0:\n",
    "            vid_data=np.zeros((video_tensor.shape[0],gaussian_frame.shape[0],gaussian_frame.shape[1],3))\n",
    "        vid_data[i]=gaussian_frame\n",
    "    return vid_data\n",
    "\n",
    "def temporal_ideal_filter(tensor,low,high,fps,axis=0):\n",
    "    fft=fftpack.fft(tensor,axis=axis)\n",
    "    frequencies = fftpack.fftfreq(tensor.shape[0], d=1.0 / fps)\n",
    "    bound_low = (np.abs(frequencies - low)).argmin()\n",
    "    bound_high = (np.abs(frequencies - high)).argmin()\n",
    "    fft[:bound_low] = 0\n",
    "    fft[bound_high:-bound_high] = 0\n",
    "    fft[-bound_low:] = 0\n",
    "    iff=fftpack.ifft(fft, axis=axis)\n",
    "    return np.abs(iff)\n",
    "\n",
    "def magnify_color(video_name,low,high,levels=3,amplification=50):\n",
    "    # t,f=load_video(video_name)\n",
    "    t=vid_data\n",
    "    f=fps\n",
    "    print(f)\n",
    "    gau_video=gaussian_video(t,levels=levels)\n",
    "    filtered_tensor=temporal_ideal_filter(gau_video,low,high,f)\n",
    "    amplified_video=amplify_video(filtered_tensor,amplification=amplification)\n",
    "    final=reconstract_video(amplified_video,t,levels=levels)\n",
    "    print(final)\n",
    "    return final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
