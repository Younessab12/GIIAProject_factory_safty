{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "##mediapipe hand detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from imutils import paths\n",
    "import face_recognition\n",
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "  To calibrate we will record the following:\n",
    "    + skin color detection\n",
    "    + mouth movements calibration:\n",
    "      - yawning\n",
    "      - talking\n",
    "      - ...\n",
    "\"\"\"\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "class FaceRecord:\n",
    "  def __init__(self, title):\n",
    "    self.title = title\n",
    "    self.lips_record = []\n",
    "    self.left_eye_record = []\n",
    "    self.right_eye_record = []\n",
    "\n",
    "  def add_to_record(self, result):\n",
    "    self.lips_record.append(result['lips'])\n",
    "    self.left_eye_record.append(result['left_eye'])\n",
    "    self.right_eye_record.append(result['right_eye'])\n",
    "\n",
    "  def get_record(self):\n",
    "    return {\n",
    "      \"lips\": self.lips_record,\n",
    "      \"left_eye\": self.left_eye_record,\n",
    "      \"right_eye\": self.right_eye_record\n",
    "    }\n",
    "\n",
    "SKIN_DETECTION, STANDARD_FACE, YAWNING, TALKING, ENDED = range(5)\n",
    "\n",
    "class Calib:\n",
    "  def __init__(self, person_name=\"default\"):\n",
    "    self.owner = person_name\n",
    "    self.env_init()\n",
    "    self.start_time= time.time()\n",
    "    self.duration = 0\n",
    "    self.state = 0\n",
    "    \n",
    "    part_skin = ['forehead', 'left_cheek', 'right_cheek', 'right_hand', 'left_hand']\n",
    "    self.record = { part: [] for part in part_skin }\n",
    "    self.color = { part: None for part in part_skin }\n",
    "\n",
    "    self.face_record = {\n",
    "      STANDARD_FACE: FaceRecord(STANDARD_FACE),\n",
    "      YAWNING: FaceRecord(YAWNING),\n",
    "      TALKING: FaceRecord(TALKING)\n",
    "    }\n",
    "\n",
    "    self.messages = [\n",
    "      \"show face and hands for skin detection\",\n",
    "      \"show face in natural position\",\n",
    "      \"show face in yawning position\",\n",
    "      \"read the following text: 'The quick brown fox jumps over the lazy dog'\",\n",
    "      \"ended\"\n",
    "    ]\n",
    "    \n",
    "    self.number_frame_required = {\n",
    "      SKIN_DETECTION: 50,\n",
    "      STANDARD_FACE: 100,\n",
    "      YAWNING: 100,\n",
    "      TALKING: 250,\n",
    "      ENDED: 0\n",
    "    }\n",
    "  \n",
    "  def process(self, frame, holistic_res):\n",
    "    self.duration = time.time()- self.start_time\n",
    "\n",
    "    self.display_image_with_text(frame, f'dur : {self.duration}', \"calib\")\n",
    "    self.display_image_with_text(frame, self.messages[self.state], \"calib\", i=2)\n",
    "\n",
    "    face_landmarks = holistic_res.face_landmarks\n",
    "    pose_landmarks = holistic_res.pose_landmarks\n",
    "\n",
    "    calibrations = [\n",
    "      self.skin_color_detection,\n",
    "      self.calibrate_general,\n",
    "      self.calibrate_yawning,\n",
    "      self.calibrate_talking,\n",
    "    ]\n",
    "\n",
    "    if self.state < len(calibrations):\n",
    "      state_over = calibrations[self.state](frame, face_landmarks, pose_landmarks)\n",
    "      if state_over:\n",
    "        print(f\"state {self.state} over\")\n",
    "        self.state += 1\n",
    "        time.sleep(1)\n",
    "        print(f\"state {self.state} started\")\n",
    "        print(self.messages[self.state])\n",
    "\n",
    "    return self.get_state() == len(calibrations)\n",
    "\n",
    "  def train_model(self):\n",
    "    \"\"\"\n",
    "        this fct will train the model and save the encodings in a pickle file\n",
    "        param:\n",
    "             path_to_imgs_folder: path to the folder containing the images\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"[INFO] start processing faces...\")\n",
    "    imagePaths = list(paths.list_images(self.face_recognition_dataset_folder))\n",
    "\n",
    "    # initialize the list of known encodings and known names\n",
    "    knownEncodings = []\n",
    "    knownNames = []\n",
    "\n",
    "    # loop over the image paths\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        # extract the person name from the image path\n",
    "        print(\"[INFO] processing image {}/{}\".format(i + 1,\n",
    "            len(imagePaths)))\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "        # load the input image and convert it from RGB (OpenCV ordering)\n",
    "        # to dlib ordering (RGB)\n",
    "        image = cv2.imread(imagePath)\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # detect the (x, y)-coordinates of the bounding boxes\n",
    "        # corresponding to each face in the input image\n",
    "        boxes = face_recognition.face_locations(rgb, model=\"hog\")\n",
    "\n",
    "        # compute the facial embedding for the face\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "\n",
    "        # loop over the encodings\n",
    "        for encoding in encodings:\n",
    "            # add each encoding + name to our set of known names and\n",
    "            # encodings\n",
    "            knownEncodings.append(encoding)\n",
    "            knownNames.append(name)\n",
    "\n",
    "    # dump the facial encodings + names to disk\n",
    "    print(\"[INFO] serializing encodings...\")\n",
    "    data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "    f = open(f\"{self.folder}/encodings.pickle\", \"wb\")\n",
    "    f.write(pickle.dumps(data))\n",
    "    f.close()\n",
    "\n",
    "  def get_results(self):\n",
    "    return {\n",
    "      'person': self.owner,\n",
    "      'record_time' : self.start_time,\n",
    "      \"duration\": self.duration,\n",
    "      SKIN_DETECTION: self.color,\n",
    "      YAWNING: self.face_record[YAWNING].get_record(),\n",
    "      TALKING: self.face_record[TALKING].get_record(),\n",
    "      STANDARD_FACE: self.face_record[STANDARD_FACE].get_record()\n",
    "    }\n",
    "\n",
    "  def skin_color_detection(self, frame, face_landmarks, pose_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and face_landmarks\n",
    "      Crop the forehead area and cheeks and get the average color for each\n",
    "      return the array of the average colors\n",
    "    \"\"\"\n",
    "\n",
    "    skin_color = self.get_skin_color_from_frame(frame, face_landmarks, pose_landmarks)\n",
    "    for part, part_color  in skin_color.items():\n",
    "      if part_color is not None and self.color[part] is None:\n",
    "        self.record[part].append(part_color)\n",
    "\n",
    "    for part, colors in self.record.items():\n",
    "      if len(colors) > self.number_frame_required[SKIN_DETECTION]:\n",
    "        self.color[part] = np.average(colors, axis=0)\n",
    "\n",
    "    print(self.color)\n",
    "\n",
    "    return not any([color is None for color in self.color.values()])\n",
    "    \n",
    "  def get_skin_color_from_frame(self, frame, face_landmarks, pose_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and face_landmarks\n",
    "      Crop the forehead area and cheeks and get the average color for each\n",
    "      return the array of the average colors\n",
    "    \"\"\"\n",
    "    record = {}\n",
    "    forehead, left_cheek, right_cheek, right_hand, left_hand = None, None, None, None, None\n",
    "\n",
    "    if face_landmarks:\n",
    "      forehead = self.crop_forehead(frame, face_landmarks)\n",
    "      left_cheek = self.crop_left_cheek(frame, face_landmarks)\n",
    "      right_cheek = self.crop_right_cheek(frame, face_landmarks)\n",
    "    \n",
    "    if pose_landmarks:\n",
    "      right_hand = self.crop_right_hand(frame, pose_landmarks)\n",
    "      left_hand = self.crop_left_hand(frame, pose_landmarks)\n",
    "\n",
    "    record['forehead'] = self.get_average_color(forehead)\n",
    "    record['left_cheek'] = self.get_average_color(left_cheek)\n",
    "    record['right_cheek'] = self.get_average_color(right_cheek)\n",
    "    record['right_hand'] = self.get_average_color(right_hand)\n",
    "    record['left_hand'] = self.get_average_color(left_hand)\n",
    "\n",
    "    return record\n",
    "\n",
    "  def calibrate_yawning(self, frame, face_landmarks, pose_landmarks):\n",
    "    return self.calibrate_face_action(face_landmarks, YAWNING)\n",
    "\n",
    "  def calibrate_talking(self, frame, face_landmarks, pose_landmarks):\n",
    "    return self.calibrate_face_action(face_landmarks, TALKING)\n",
    "\n",
    "  def calibrate_general(self, frame, face_landmarks, pose_landmarks):\n",
    "    self.save_image_recogniton(frame)\n",
    "    return self.calibrate_face_action(face_landmarks, STANDARD_FACE)\n",
    "\n",
    "  def calibrate_face_action(self, face_landmarks, action):\n",
    "    if not face_landmarks: return False\n",
    "    self.face_record[action].add_to_record(self.get_eyes_lips_relative_distance(face_landmarks))\n",
    "    return len(self.face_record[action].lips_record) > self.number_frame_required[action]\n",
    "\n",
    "  def crop_forehead(self, frame, face_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and face_landmarks\n",
    "      Crop the forehead area and return it\n",
    "    \"\"\"\n",
    "    forehead_landmark = face_landmarks.landmark[151]\n",
    "    forehead = self.crop_part_from_image(frame, forehead_landmark, 20)\n",
    "    return forehead\n",
    "  \n",
    "  def crop_left_cheek(self, frame, face_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and face_landmarks\n",
    "      Crop the left cheek area and return it\n",
    "    \"\"\"\n",
    "    left_cheek_landmark = face_landmarks.landmark[118]\n",
    "    left_cheek = self.crop_part_from_image(frame, left_cheek_landmark, 20)\n",
    "    return left_cheek\n",
    "  \n",
    "  def crop_right_cheek(self, frame, face_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and face_landmarks\n",
    "      Crop the right cheek area and return it\n",
    "    \"\"\"\n",
    "    right_cheek_landmark = face_landmarks.landmark[348]\n",
    "    right_cheek = self.crop_part_from_image(frame, right_cheek_landmark, 20)\n",
    "    return right_cheek\n",
    "  \n",
    "  def crop_right_hand(self, frame, pose_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and pose_landmarks\n",
    "      Crop the right hand area and return it\n",
    "    \"\"\"\n",
    "    right_hand_landmark = pose_landmarks.landmark[15]\n",
    "    right_hand = self.crop_part_from_image(frame, right_hand_landmark, 20)\n",
    "    return right_hand\n",
    "  \n",
    "  def crop_left_hand(self, frame, pose_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and pose_landmarks\n",
    "      Crop the left hand area and return it\n",
    "    \"\"\"\n",
    "    left_hand_landmark = pose_landmarks.landmark[16]\n",
    "    left_hand = self.crop_part_from_image(frame, left_hand_landmark, 20)\n",
    "    return left_hand\n",
    "\n",
    "  def crop_part_from_image(self, frame, point, width):\n",
    "    \"\"\"\n",
    "      Get frame and point\n",
    "      Crop the area around the point and return it\n",
    "    \"\"\"\n",
    "    part = frame[\n",
    "      int(point.y * frame.shape[0])-width: int(point.y * frame.shape[0]+width),\n",
    "      int(point.x * frame.shape[1])-width: int(point.x * frame.shape[1]+width)\n",
    "    ]\n",
    "    if len(part) <= 0 or len(part[0]) <= 0: return None\n",
    "    return part\n",
    "\n",
    "  def get_average_color(self, frame):\n",
    "    if frame is None: return None\n",
    "    avg_color_per_row = np.average(frame, axis=0)\n",
    "    avg_color = np.average(avg_color_per_row, axis=0)\n",
    "    return avg_color\n",
    "\n",
    "  def get_state(self):\n",
    "    pass\n",
    "\n",
    "  def display_image_with_text(self, img, text, title, i=1):\n",
    "    cv2.putText(\n",
    "      img = img,\n",
    "      text = text,\n",
    "      org = (10, 10+i*20),\n",
    "      fontFace = cv2.FONT_HERSHEY_DUPLEX,\n",
    "      fontScale = 1.0,\n",
    "      color = (125, 246, 55),\n",
    "      thickness = 1\n",
    "    )\n",
    "    cv2.imshow(title, img)\n",
    "\n",
    "  def calculate_distance(self, point1, point2):\n",
    "    return math.sqrt((point1.x - point2.x)**2 + (point1.y - point2.y)**2+(point1.z - point2.z)**2)\n",
    "\n",
    "  def get_eyes_lips_relative_distance(self, face_landmarks):\n",
    "    if not face_landmarks:\n",
    "        return {\n",
    "            \"lips\": -1,\n",
    "            \"left_eye\": -1,\n",
    "            \"right_eye\": -1\n",
    "        }\n",
    "\n",
    "    #relevant points for lips, left eye, and right eye\n",
    "    upper_lip = face_landmarks.landmark[13]\n",
    "    bottom_lip = face_landmarks.landmark[14]\n",
    "    upper_left_eye_point = face_landmarks.landmark[386]\n",
    "    bottom_left_eye_point = face_landmarks.landmark[374]\n",
    "    upper_right_eye_points = face_landmarks.landmark[159]\n",
    "    bottom_right_eye_points = face_landmarks.landmark[145]\n",
    "    upper_face = face_landmarks.landmark[10]\n",
    "    bottom_face = face_landmarks.landmark[152]\n",
    "\n",
    "    #Relative distances\n",
    "    lips_distance = self.calculate_distance(upper_lip, bottom_lip) / self.calculate_distance(upper_face, bottom_face)\n",
    "    left_eye_distance = self.calculate_distance(upper_left_eye_point, bottom_left_eye_point) / self.calculate_distance(upper_face, bottom_face)\n",
    "    right_eye_distance = self.calculate_distance(upper_right_eye_points, bottom_right_eye_points) / self.calculate_distance(upper_face, bottom_face)\n",
    "\n",
    "    return {\n",
    "        \"lips\": lips_distance,\n",
    "        \"left_eye\": left_eye_distance,\n",
    "        \"right_eye\": right_eye_distance\n",
    "    }\n",
    "\n",
    "  def save_image_recogniton(self, frame):\n",
    "    cv2.imwrite(f'{self.face_recognition_folder}/{time.time()}.jpg', frame)\n",
    "\n",
    "  def env_init(self):\n",
    "    folder = 'calib_records'\n",
    "    if not os.path.exists(folder):\n",
    "      os.makedirs(folder)\n",
    "\n",
    "    face_recognition_folder = f'{folder}/data/{self.owner}'\n",
    "    if not os.path.exists(face_recognition_folder):\n",
    "      os.makedirs(face_recognition_folder)\n",
    "\n",
    "    self.folder = folder\n",
    "    self.face_recognition_dataset_folder = f'{folder}/data'\n",
    "    self.face_recognition_folder = face_recognition_folder\n",
    "    self.record_file = f'{folder}/calibration_{self.owner}'\n",
    "\n",
    "  def export_json(self):\n",
    "    data = self.get_results()\n",
    "    with open(f'{self.record_file}.json', 'w') as outfile:\n",
    "      json.dump(data, outfile, cls=NumpyEncoder)\n",
    "\n",
    "calibration_process = Calib(\"amine_firdawsi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': None, 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "{'forehead': array([ 83.39917892,  98.23974265, 151.16648284]), 'left_cheek': array([106.36647059, 116.50367647, 157.55324755]), 'right_cheek': array([ 88.0389951 , 101.21963235, 151.00769608]), 'right_hand': array([102.15768081, 113.09568206, 154.93471003]), 'left_hand': array([ 76.51623696,  85.86943297, 117.13582573])}\n",
      "state 0 over\n",
      "state 1 started\n",
      "show face in natural position\n",
      "state 1 over\n",
      "state 2 started\n",
      "show face in yawning position\n",
      "state 2 over\n",
      "state 3 started\n",
      "read the following text: 'The quick brown fox jumps over the lazy dog'\n",
      "state 3 over\n",
      "state 4 started\n",
      "ended\n",
      "[INFO] start processing faces...\n",
      "[INFO] processing image 1/202\n",
      "[INFO] processing image 2/202\n",
      "[INFO] processing image 3/202\n",
      "[INFO] processing image 4/202\n",
      "[INFO] processing image 5/202\n",
      "[INFO] processing image 6/202\n",
      "[INFO] processing image 7/202\n",
      "[INFO] processing image 8/202\n",
      "[INFO] processing image 9/202\n",
      "[INFO] processing image 10/202\n",
      "[INFO] processing image 11/202\n",
      "[INFO] processing image 12/202\n",
      "[INFO] processing image 13/202\n",
      "[INFO] processing image 14/202\n",
      "[INFO] processing image 15/202\n",
      "[INFO] processing image 16/202\n",
      "[INFO] processing image 17/202\n",
      "[INFO] processing image 18/202\n",
      "[INFO] processing image 19/202\n",
      "[INFO] processing image 20/202\n",
      "[INFO] processing image 21/202\n",
      "[INFO] processing image 22/202\n",
      "[INFO] processing image 23/202\n",
      "[INFO] processing image 24/202\n",
      "[INFO] processing image 25/202\n",
      "[INFO] processing image 26/202\n",
      "[INFO] processing image 27/202\n",
      "[INFO] processing image 28/202\n",
      "[INFO] processing image 29/202\n",
      "[INFO] processing image 30/202\n",
      "[INFO] processing image 31/202\n",
      "[INFO] processing image 32/202\n",
      "[INFO] processing image 33/202\n",
      "[INFO] processing image 34/202\n",
      "[INFO] processing image 35/202\n",
      "[INFO] processing image 36/202\n",
      "[INFO] processing image 37/202\n",
      "[INFO] processing image 38/202\n",
      "[INFO] processing image 39/202\n",
      "[INFO] processing image 40/202\n",
      "[INFO] processing image 41/202\n",
      "[INFO] processing image 42/202\n",
      "[INFO] processing image 43/202\n",
      "[INFO] processing image 44/202\n",
      "[INFO] processing image 45/202\n",
      "[INFO] processing image 46/202\n",
      "[INFO] processing image 47/202\n",
      "[INFO] processing image 48/202\n",
      "[INFO] processing image 49/202\n",
      "[INFO] processing image 50/202\n",
      "[INFO] processing image 51/202\n",
      "[INFO] processing image 52/202\n",
      "[INFO] processing image 53/202\n",
      "[INFO] processing image 54/202\n",
      "[INFO] processing image 55/202\n",
      "[INFO] processing image 56/202\n",
      "[INFO] processing image 57/202\n",
      "[INFO] processing image 58/202\n",
      "[INFO] processing image 59/202\n",
      "[INFO] processing image 60/202\n",
      "[INFO] processing image 61/202\n",
      "[INFO] processing image 62/202\n",
      "[INFO] processing image 63/202\n",
      "[INFO] processing image 64/202\n",
      "[INFO] processing image 65/202\n",
      "[INFO] processing image 66/202\n",
      "[INFO] processing image 67/202\n",
      "[INFO] processing image 68/202\n",
      "[INFO] processing image 69/202\n",
      "[INFO] processing image 70/202\n",
      "[INFO] processing image 71/202\n",
      "[INFO] processing image 72/202\n",
      "[INFO] processing image 73/202\n",
      "[INFO] processing image 74/202\n",
      "[INFO] processing image 75/202\n",
      "[INFO] processing image 76/202\n",
      "[INFO] processing image 77/202\n",
      "[INFO] processing image 78/202\n",
      "[INFO] processing image 79/202\n",
      "[INFO] processing image 80/202\n",
      "[INFO] processing image 81/202\n",
      "[INFO] processing image 82/202\n",
      "[INFO] processing image 83/202\n",
      "[INFO] processing image 84/202\n",
      "[INFO] processing image 85/202\n",
      "[INFO] processing image 86/202\n",
      "[INFO] processing image 87/202\n",
      "[INFO] processing image 88/202\n",
      "[INFO] processing image 89/202\n",
      "[INFO] processing image 90/202\n",
      "[INFO] processing image 91/202\n",
      "[INFO] processing image 92/202\n",
      "[INFO] processing image 93/202\n",
      "[INFO] processing image 94/202\n",
      "[INFO] processing image 95/202\n",
      "[INFO] processing image 96/202\n",
      "[INFO] processing image 97/202\n",
      "[INFO] processing image 98/202\n",
      "[INFO] processing image 99/202\n",
      "[INFO] processing image 100/202\n",
      "[INFO] processing image 101/202\n",
      "[INFO] processing image 102/202\n",
      "[INFO] processing image 103/202\n",
      "[INFO] processing image 104/202\n",
      "[INFO] processing image 105/202\n",
      "[INFO] processing image 106/202\n",
      "[INFO] processing image 107/202\n",
      "[INFO] processing image 108/202\n",
      "[INFO] processing image 109/202\n",
      "[INFO] processing image 110/202\n",
      "[INFO] processing image 111/202\n",
      "[INFO] processing image 112/202\n",
      "[INFO] processing image 113/202\n",
      "[INFO] processing image 114/202\n",
      "[INFO] processing image 115/202\n",
      "[INFO] processing image 116/202\n",
      "[INFO] processing image 117/202\n",
      "[INFO] processing image 118/202\n",
      "[INFO] processing image 119/202\n",
      "[INFO] processing image 120/202\n",
      "[INFO] processing image 121/202\n",
      "[INFO] processing image 122/202\n",
      "[INFO] processing image 123/202\n",
      "[INFO] processing image 124/202\n",
      "[INFO] processing image 125/202\n",
      "[INFO] processing image 126/202\n",
      "[INFO] processing image 127/202\n",
      "[INFO] processing image 128/202\n",
      "[INFO] processing image 129/202\n",
      "[INFO] processing image 130/202\n",
      "[INFO] processing image 131/202\n",
      "[INFO] processing image 132/202\n",
      "[INFO] processing image 133/202\n",
      "[INFO] processing image 134/202\n",
      "[INFO] processing image 135/202\n",
      "[INFO] processing image 136/202\n",
      "[INFO] processing image 137/202\n",
      "[INFO] processing image 138/202\n",
      "[INFO] processing image 139/202\n",
      "[INFO] processing image 140/202\n",
      "[INFO] processing image 141/202\n",
      "[INFO] processing image 142/202\n",
      "[INFO] processing image 143/202\n",
      "[INFO] processing image 144/202\n",
      "[INFO] processing image 145/202\n",
      "[INFO] processing image 146/202\n",
      "[INFO] processing image 147/202\n",
      "[INFO] processing image 148/202\n",
      "[INFO] processing image 149/202\n",
      "[INFO] processing image 150/202\n",
      "[INFO] processing image 151/202\n",
      "[INFO] processing image 152/202\n",
      "[INFO] processing image 153/202\n",
      "[INFO] processing image 154/202\n",
      "[INFO] processing image 155/202\n",
      "[INFO] processing image 156/202\n",
      "[INFO] processing image 157/202\n",
      "[INFO] processing image 158/202\n",
      "[INFO] processing image 159/202\n",
      "[INFO] processing image 160/202\n",
      "[INFO] processing image 161/202\n",
      "[INFO] processing image 162/202\n",
      "[INFO] processing image 163/202\n",
      "[INFO] processing image 164/202\n",
      "[INFO] processing image 165/202\n",
      "[INFO] processing image 166/202\n",
      "[INFO] processing image 167/202\n",
      "[INFO] processing image 168/202\n",
      "[INFO] processing image 169/202\n",
      "[INFO] processing image 170/202\n",
      "[INFO] processing image 171/202\n",
      "[INFO] processing image 172/202\n",
      "[INFO] processing image 173/202\n",
      "[INFO] processing image 174/202\n",
      "[INFO] processing image 175/202\n",
      "[INFO] processing image 176/202\n",
      "[INFO] processing image 177/202\n",
      "[INFO] processing image 178/202\n",
      "[INFO] processing image 179/202\n",
      "[INFO] processing image 180/202\n",
      "[INFO] processing image 181/202\n",
      "[INFO] processing image 182/202\n",
      "[INFO] processing image 183/202\n",
      "[INFO] processing image 184/202\n",
      "[INFO] processing image 185/202\n",
      "[INFO] processing image 186/202\n",
      "[INFO] processing image 187/202\n",
      "[INFO] processing image 188/202\n",
      "[INFO] processing image 189/202\n",
      "[INFO] processing image 190/202\n",
      "[INFO] processing image 191/202\n",
      "[INFO] processing image 192/202\n",
      "[INFO] processing image 193/202\n",
      "[INFO] processing image 194/202\n",
      "[INFO] processing image 195/202\n",
      "[INFO] processing image 196/202\n",
      "[INFO] processing image 197/202\n",
      "[INFO] processing image 198/202\n",
      "[INFO] processing image 199/202\n",
      "[INFO] processing image 200/202\n",
      "[INFO] processing image 201/202\n",
      "[INFO] processing image 202/202\n",
      "[INFO] serializing encodings...\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.flip(img, 1)\n",
    "        res = holistic.process(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        calibration_process.process(frame=img, holistic_res=res)\n",
    "\n",
    "        if calibration_process.state == ENDED:\n",
    "          break\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "calibration_process.export_json()\n",
    "calibration_process.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Calib' object has no attribute 'train_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Amine\\Desktop\\AI\\GIIAProject_factory_safty\\profiling&calib\\main.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Amine/Desktop/AI/GIIAProject_factory_safty/profiling%26calib/main.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m calibration_process\u001b[39m.\u001b[39;49mtrain_model()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Calib' object has no attribute 'train_model'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
