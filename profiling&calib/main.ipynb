{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dell Latitude 5420\\.conda\\envs\\Factory_Safety\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##mediapipe hand detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from imutils import paths\n",
    "import face_recognition\n",
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "  To calibrate we will record the following:\n",
    "    + skin color detection\n",
    "    + mouth movements calibration:\n",
    "      - yawning\n",
    "      - talking\n",
    "      - ...\n",
    "\"\"\"\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "class FaceRecord:\n",
    "  def __init__(self, title):\n",
    "    self.title = title\n",
    "    self.lips_record = []\n",
    "    self.left_eye_record = []\n",
    "    self.right_eye_record = []\n",
    "\n",
    "  def add_to_record(self, result):\n",
    "    self.lips_record.append(result['lips'])\n",
    "    self.left_eye_record.append(result['left_eye'])\n",
    "    self.right_eye_record.append(result['right_eye'])\n",
    "\n",
    "  def get_record(self):\n",
    "    return {\n",
    "      \"lips\": self.lips_record,\n",
    "      \"left_eye\": self.left_eye_record,\n",
    "      \"right_eye\": self.right_eye_record\n",
    "    }\n",
    "\n",
    "SKIN_DETECTION, STANDARD_FACE, YAWNING, TALKING, ENDED = range(5)\n",
    "\n",
    "class Calib:\n",
    "  def __init__(self, person_name=\"default\"):\n",
    "    self.owner = person_name\n",
    "    self.env_init()\n",
    "    self.start_time= time.time()\n",
    "    self.duration = 0\n",
    "    self.state = 0\n",
    "    \n",
    "    part_skin = ['forehead', 'left_cheek', 'right_cheek', 'right_hand', 'left_hand']\n",
    "    self.record = { part: [] for part in part_skin }\n",
    "    self.color = { part: None for part in part_skin }\n",
    "\n",
    "    self.face_record = {\n",
    "      STANDARD_FACE: FaceRecord(STANDARD_FACE),\n",
    "      YAWNING: FaceRecord(YAWNING),\n",
    "      TALKING: FaceRecord(TALKING)\n",
    "    }\n",
    "\n",
    "    self.messages = [\n",
    "      \"show face and hands for skin detection\",\n",
    "      \"show face in natural position\",\n",
    "      \"show face in yawning position\",\n",
    "      \"read the following text: 'The quick brown fox jumps over the lazy dog'\",\n",
    "      \"ended\"\n",
    "    ]\n",
    "    \n",
    "    self.number_frame_required = {\n",
    "      SKIN_DETECTION: 50,\n",
    "      STANDARD_FACE: 100,\n",
    "      YAWNING: 100,\n",
    "      TALKING: 250,\n",
    "      ENDED: 0\n",
    "    }\n",
    "  \n",
    "  def process(self, frame, holistic_res):\n",
    "    self.duration = time.time()- self.start_time\n",
    "\n",
    "    self.display_image_with_text(frame, f'dur : {self.duration}', \"calib\")\n",
    "    self.display_image_with_text(frame, self.messages[self.state], \"calib\", i=2)\n",
    "\n",
    "    face_landmarks = holistic_res.face_landmarks\n",
    "    pose_landmarks = holistic_res.pose_landmarks\n",
    "\n",
    "    calibrations = [\n",
    "      self.skin_color_detection,\n",
    "      self.calibrate_general,\n",
    "      self.calibrate_yawning,\n",
    "      self.calibrate_talking,\n",
    "    ]\n",
    "\n",
    "    if self.state < len(calibrations):\n",
    "      state_over = calibrations[self.state](frame, face_landmarks, pose_landmarks)\n",
    "      if state_over:\n",
    "        print(f\"state {self.state} over\")\n",
    "        self.state += 1\n",
    "        time.sleep(1)\n",
    "        print(f\"state {self.state} started\")\n",
    "        print(self.messages[self.state])\n",
    "\n",
    "    return self.get_state() == len(calibrations)\n",
    "\n",
    "  def train_model(self):\n",
    "    \"\"\"\n",
    "        this fct will train the model and save the encodings in a pickle file\n",
    "        param:\n",
    "             path_to_imgs_folder: path to the folder containing the images\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"[INFO] start processing faces...\")\n",
    "    imagePaths = list(paths.list_images(self.face_recognition_dataset_folder))\n",
    "\n",
    "    # initialize the list of known encodings and known names\n",
    "    knownEncodings = []\n",
    "    knownNames = []\n",
    "\n",
    "    # loop over the image paths\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        # extract the person name from the image path\n",
    "        print(\"[INFO] processing image {}/{}\".format(i + 1,\n",
    "            len(imagePaths)))\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "        # load the input image and convert it from RGB (OpenCV ordering)\n",
    "        # to dlib ordering (RGB)\n",
    "        image = cv2.imread(imagePath)\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # detect the (x, y)-coordinates of the bounding boxes\n",
    "        # corresponding to each face in the input image\n",
    "        boxes = face_recognition.face_locations(rgb, model=\"hog\")\n",
    "\n",
    "        # compute the facial embedding for the face\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "\n",
    "        # loop over the encodings\n",
    "        for encoding in encodings:\n",
    "            # add each encoding + name to our set of known names and\n",
    "            # encodings\n",
    "            knownEncodings.append(encoding)\n",
    "            knownNames.append(name)\n",
    "\n",
    "    # dump the facial encodings + names to disk\n",
    "    print(\"[INFO] serializing encodings...\")\n",
    "    data = {\"encodings\": knownEncodings, \"names\": knownNames}\n",
    "    f = open(f\"{self.folder}/encodings.pickle\", \"wb\")\n",
    "    f.write(pickle.dumps(data))\n",
    "    f.close()\n",
    "\n",
    "  def get_results(self):\n",
    "    return {\n",
    "      'person': self.owner,\n",
    "      'record_time' : self.start_time,\n",
    "      \"duration\": self.duration,\n",
    "      SKIN_DETECTION: self.color,\n",
    "      YAWNING: self.face_record[YAWNING].get_record(),\n",
    "      TALKING: self.face_record[TALKING].get_record(),\n",
    "      STANDARD_FACE: self.face_record[STANDARD_FACE].get_record()\n",
    "    }\n",
    "\n",
    "  def skin_color_detection(self, frame, face_landmarks, pose_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and face_landmarks\n",
    "      Crop the forehead area and cheeks and get the average color for each\n",
    "      return the array of the average colors\n",
    "    \"\"\"\n",
    "\n",
    "    skin_color = self.get_skin_color_from_frame(frame, face_landmarks, pose_landmarks)\n",
    "    for part, part_color  in skin_color.items():\n",
    "      if part_color is not None and self.color[part] is None:\n",
    "        self.record[part].append(part_color)\n",
    "\n",
    "    for part, colors in self.record.items():\n",
    "      if len(colors) > self.number_frame_required[SKIN_DETECTION]:\n",
    "        self.color[part] = np.average(colors, axis=0)\n",
    "\n",
    "    print(self.color)\n",
    "\n",
    "    return not any([color is None for color in self.color.values()])\n",
    "    \n",
    "  def get_skin_color_from_frame(self, frame, face_landmarks, pose_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and face_landmarks\n",
    "      Crop the forehead area and cheeks and get the average color for each\n",
    "      return the array of the average colors\n",
    "    \"\"\"\n",
    "    record = {}\n",
    "    forehead, left_cheek, right_cheek, right_hand, left_hand = None, None, None, None, None\n",
    "\n",
    "    if face_landmarks:\n",
    "      forehead = self.crop_forehead(frame, face_landmarks)\n",
    "      left_cheek = self.crop_left_cheek(frame, face_landmarks)\n",
    "      right_cheek = self.crop_right_cheek(frame, face_landmarks)\n",
    "    \n",
    "    if pose_landmarks:\n",
    "      right_hand = self.crop_right_hand(frame, pose_landmarks)\n",
    "      left_hand = self.crop_left_hand(frame, pose_landmarks)\n",
    "\n",
    "    record['forehead'] = self.get_average_color(forehead)\n",
    "    record['left_cheek'] = self.get_average_color(left_cheek)\n",
    "    record['right_cheek'] = self.get_average_color(right_cheek)\n",
    "    record['right_hand'] = self.get_average_color(right_hand)\n",
    "    record['left_hand'] = self.get_average_color(left_hand)\n",
    "\n",
    "    return record\n",
    "\n",
    "  def calibrate_yawning(self, frame, face_landmarks, pose_landmarks):\n",
    "    return self.calibrate_face_action(face_landmarks, YAWNING)\n",
    "\n",
    "  def calibrate_talking(self, frame, face_landmarks, pose_landmarks):\n",
    "    return self.calibrate_face_action(face_landmarks, TALKING)\n",
    "\n",
    "  def calibrate_general(self, frame, face_landmarks, pose_landmarks):\n",
    "    self.save_image_recogniton(frame)\n",
    "    return self.calibrate_face_action(face_landmarks, STANDARD_FACE)\n",
    "\n",
    "  def calibrate_face_action(self, face_landmarks, action):\n",
    "    if not face_landmarks: return False\n",
    "    self.face_record[action].add_to_record(self.get_eyes_lips_relative_distance(face_landmarks))\n",
    "    return len(self.face_record[action].lips_record) > self.number_frame_required[action]\n",
    "\n",
    "  def crop_forehead(self, frame, face_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and face_landmarks\n",
    "      Crop the forehead area and return it\n",
    "    \"\"\"\n",
    "    forehead_landmark = face_landmarks.landmark[151]\n",
    "    forehead = self.crop_part_from_image(frame, forehead_landmark, 20)\n",
    "    return forehead\n",
    "  \n",
    "  def crop_left_cheek(self, frame, face_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and face_landmarks\n",
    "      Crop the left cheek area and return it\n",
    "    \"\"\"\n",
    "    left_cheek_landmark = face_landmarks.landmark[118]\n",
    "    left_cheek = self.crop_part_from_image(frame, left_cheek_landmark, 20)\n",
    "    return left_cheek\n",
    "  \n",
    "  def crop_right_cheek(self, frame, face_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and face_landmarks\n",
    "      Crop the right cheek area and return it\n",
    "    \"\"\"\n",
    "    right_cheek_landmark = face_landmarks.landmark[348]\n",
    "    right_cheek = self.crop_part_from_image(frame, right_cheek_landmark, 20)\n",
    "    return right_cheek\n",
    "  \n",
    "  def crop_right_hand(self, frame, pose_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and pose_landmarks\n",
    "      Crop the right hand area and return it\n",
    "    \"\"\"\n",
    "    right_hand_landmark = pose_landmarks.landmark[15]\n",
    "    right_hand = self.crop_part_from_image(frame, right_hand_landmark, 20)\n",
    "    return right_hand\n",
    "  \n",
    "  def crop_left_hand(self, frame, pose_landmarks):\n",
    "    \"\"\"\n",
    "      Get frame and pose_landmarks\n",
    "      Crop the left hand area and return it\n",
    "    \"\"\"\n",
    "    left_hand_landmark = pose_landmarks.landmark[16]\n",
    "    left_hand = self.crop_part_from_image(frame, left_hand_landmark, 20)\n",
    "    return left_hand\n",
    "\n",
    "  def crop_part_from_image(self, frame, point, width):\n",
    "    \"\"\"\n",
    "      Get frame and point\n",
    "      Crop the area around the point and return it\n",
    "    \"\"\"\n",
    "    part = frame[\n",
    "      int(point.y * frame.shape[0])-width: int(point.y * frame.shape[0]+width),\n",
    "      int(point.x * frame.shape[1])-width: int(point.x * frame.shape[1]+width)\n",
    "    ]\n",
    "    if len(part) <= 0 or len(part[0]) <= 0: return None\n",
    "    return part\n",
    "\n",
    "  def get_average_color(self, frame):\n",
    "    if frame is None: return None\n",
    "    avg_color_per_row = np.average(frame, axis=0)\n",
    "    avg_color = np.average(avg_color_per_row, axis=0)\n",
    "    return avg_color\n",
    "\n",
    "  def get_state(self):\n",
    "    pass\n",
    "\n",
    "  def display_image_with_text(self, img, text, title, i=1):\n",
    "    cv2.putText(\n",
    "      img = img,\n",
    "      text = text,\n",
    "      org = (10, 10+i*20),\n",
    "      fontFace = cv2.FONT_HERSHEY_DUPLEX,\n",
    "      fontScale = 1.0,\n",
    "      color = (125, 246, 55),\n",
    "      thickness = 1\n",
    "    )\n",
    "    cv2.imshow(title, img)\n",
    "\n",
    "  def calculate_distance(self, point1, point2):\n",
    "    return math.sqrt((point1.x - point2.x)**2 + (point1.y - point2.y)**2+(point1.z - point2.z)**2)\n",
    "\n",
    "  def get_eyes_lips_relative_distance(self, face_landmarks):\n",
    "    if not face_landmarks:\n",
    "        return {\n",
    "            \"lips\": -1,\n",
    "            \"left_eye\": -1,\n",
    "            \"right_eye\": -1\n",
    "        }\n",
    "\n",
    "    #relevant points for lips, left eye, and right eye\n",
    "    upper_lip = face_landmarks.landmark[13]\n",
    "    bottom_lip = face_landmarks.landmark[14]\n",
    "    upper_left_eye_point = face_landmarks.landmark[386]\n",
    "    bottom_left_eye_point = face_landmarks.landmark[374]\n",
    "    upper_right_eye_points = face_landmarks.landmark[159]\n",
    "    bottom_right_eye_points = face_landmarks.landmark[145]\n",
    "    upper_face = face_landmarks.landmark[10]\n",
    "    bottom_face = face_landmarks.landmark[152]\n",
    "\n",
    "    #Relative distances\n",
    "    lips_distance = self.calculate_distance(upper_lip, bottom_lip) / self.calculate_distance(upper_face, bottom_face)\n",
    "    left_eye_distance = self.calculate_distance(upper_left_eye_point, bottom_left_eye_point) / self.calculate_distance(upper_face, bottom_face)\n",
    "    right_eye_distance = self.calculate_distance(upper_right_eye_points, bottom_right_eye_points) / self.calculate_distance(upper_face, bottom_face)\n",
    "\n",
    "    return {\n",
    "        \"lips\": lips_distance,\n",
    "        \"left_eye\": left_eye_distance,\n",
    "        \"right_eye\": right_eye_distance\n",
    "    }\n",
    "\n",
    "  def save_image_recogniton(self, frame):\n",
    "    cv2.imwrite(f'{self.face_recognition_folder}/{time.time()}.jpg', frame)\n",
    "\n",
    "  def env_init(self):\n",
    "    folder = 'calib_records'\n",
    "    if not os.path.exists(folder):\n",
    "      os.makedirs(folder)\n",
    "\n",
    "    face_recognition_folder = f'{folder}/data/{self.owner}'\n",
    "    if not os.path.exists(face_recognition_folder):\n",
    "      os.makedirs(face_recognition_folder)\n",
    "\n",
    "    self.folder = folder\n",
    "    self.face_recognition_dataset_folder = f'{folder}/data'\n",
    "    self.face_recognition_folder = face_recognition_folder\n",
    "    self.record_file = f'{folder}/calibration_{self.owner}'\n",
    "\n",
    "  def export_json(self):\n",
    "    data = self.get_results()\n",
    "    with open(f'{self.record_file}.json', 'w') as outfile:\n",
    "      json.dump(data, outfile, cls=NumpyEncoder)\n",
    "\n",
    "calibration_process = Calib(input(\"Enter your name: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': None, 'left_cheek': None, 'right_cheek': None, 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': None, 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': array([125.41447304, 129.87583333, 155.21920343]), 'left_hand': None}\n",
      "{'forehead': array([102.359375  , 107.41261029, 137.62920343]), 'left_cheek': array([141.63933824, 143.44819853, 161.97653186]), 'right_cheek': array([103.76055147,  93.24161765, 107.64258578]), 'right_hand': array([125.41447304, 129.87583333, 155.21920343]), 'left_hand': array([ 79.50072304,  81.74534314, 101.07387255])}\n",
      "state 0 over\n",
      "state 1 started\n",
      "show face in natural position\n",
      "state 1 over\n",
      "state 2 started\n",
      "show face in yawning position\n",
      "state 2 over\n",
      "state 3 started\n",
      "read the following text: 'The quick brown fox jumps over the lazy dog'\n",
      "state 3 over\n",
      "state 4 started\n",
      "ended\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.flip(img, 1)\n",
    "        res = holistic.process(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        calibration_process.process(frame=img, holistic_res=res)\n",
    "\n",
    "        if calibration_process.state == ENDED:\n",
    "          break\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "calibration_process.export_json()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] start processing faces...\n",
      "[INFO] processing image 1/404\n",
      "[INFO] processing image 2/404\n",
      "[INFO] processing image 3/404\n",
      "[INFO] processing image 4/404\n",
      "[INFO] processing image 5/404\n",
      "[INFO] processing image 6/404\n",
      "[INFO] processing image 7/404\n",
      "[INFO] processing image 8/404\n",
      "[INFO] processing image 9/404\n",
      "[INFO] processing image 10/404\n",
      "[INFO] processing image 11/404\n",
      "[INFO] processing image 12/404\n",
      "[INFO] processing image 13/404\n",
      "[INFO] processing image 14/404\n",
      "[INFO] processing image 15/404\n",
      "[INFO] processing image 16/404\n",
      "[INFO] processing image 17/404\n",
      "[INFO] processing image 18/404\n",
      "[INFO] processing image 19/404\n",
      "[INFO] processing image 20/404\n",
      "[INFO] processing image 21/404\n",
      "[INFO] processing image 22/404\n",
      "[INFO] processing image 23/404\n",
      "[INFO] processing image 24/404\n",
      "[INFO] processing image 25/404\n",
      "[INFO] processing image 26/404\n",
      "[INFO] processing image 27/404\n",
      "[INFO] processing image 28/404\n",
      "[INFO] processing image 29/404\n",
      "[INFO] processing image 30/404\n",
      "[INFO] processing image 31/404\n",
      "[INFO] processing image 32/404\n",
      "[INFO] processing image 33/404\n",
      "[INFO] processing image 34/404\n",
      "[INFO] processing image 35/404\n",
      "[INFO] processing image 36/404\n",
      "[INFO] processing image 37/404\n",
      "[INFO] processing image 38/404\n",
      "[INFO] processing image 39/404\n",
      "[INFO] processing image 40/404\n",
      "[INFO] processing image 41/404\n",
      "[INFO] processing image 42/404\n",
      "[INFO] processing image 43/404\n",
      "[INFO] processing image 44/404\n",
      "[INFO] processing image 45/404\n",
      "[INFO] processing image 46/404\n",
      "[INFO] processing image 47/404\n",
      "[INFO] processing image 48/404\n",
      "[INFO] processing image 49/404\n",
      "[INFO] processing image 50/404\n",
      "[INFO] processing image 51/404\n",
      "[INFO] processing image 52/404\n",
      "[INFO] processing image 53/404\n",
      "[INFO] processing image 54/404\n",
      "[INFO] processing image 55/404\n",
      "[INFO] processing image 56/404\n",
      "[INFO] processing image 57/404\n",
      "[INFO] processing image 58/404\n",
      "[INFO] processing image 59/404\n",
      "[INFO] processing image 60/404\n",
      "[INFO] processing image 61/404\n",
      "[INFO] processing image 62/404\n",
      "[INFO] processing image 63/404\n",
      "[INFO] processing image 64/404\n",
      "[INFO] processing image 65/404\n",
      "[INFO] processing image 66/404\n",
      "[INFO] processing image 67/404\n",
      "[INFO] processing image 68/404\n",
      "[INFO] processing image 69/404\n",
      "[INFO] processing image 70/404\n",
      "[INFO] processing image 71/404\n",
      "[INFO] processing image 72/404\n",
      "[INFO] processing image 73/404\n",
      "[INFO] processing image 74/404\n",
      "[INFO] processing image 75/404\n",
      "[INFO] processing image 76/404\n",
      "[INFO] processing image 77/404\n",
      "[INFO] processing image 78/404\n",
      "[INFO] processing image 79/404\n",
      "[INFO] processing image 80/404\n",
      "[INFO] processing image 81/404\n",
      "[INFO] processing image 82/404\n",
      "[INFO] processing image 83/404\n",
      "[INFO] processing image 84/404\n",
      "[INFO] processing image 85/404\n",
      "[INFO] processing image 86/404\n",
      "[INFO] processing image 87/404\n",
      "[INFO] processing image 88/404\n",
      "[INFO] processing image 89/404\n",
      "[INFO] processing image 90/404\n",
      "[INFO] processing image 91/404\n",
      "[INFO] processing image 92/404\n",
      "[INFO] processing image 93/404\n",
      "[INFO] processing image 94/404\n",
      "[INFO] processing image 95/404\n",
      "[INFO] processing image 96/404\n",
      "[INFO] processing image 97/404\n",
      "[INFO] processing image 98/404\n",
      "[INFO] processing image 99/404\n",
      "[INFO] processing image 100/404\n",
      "[INFO] processing image 101/404\n",
      "[INFO] processing image 102/404\n",
      "[INFO] processing image 103/404\n",
      "[INFO] processing image 104/404\n",
      "[INFO] processing image 105/404\n",
      "[INFO] processing image 106/404\n",
      "[INFO] processing image 107/404\n",
      "[INFO] processing image 108/404\n",
      "[INFO] processing image 109/404\n",
      "[INFO] processing image 110/404\n",
      "[INFO] processing image 111/404\n",
      "[INFO] processing image 112/404\n",
      "[INFO] processing image 113/404\n",
      "[INFO] processing image 114/404\n",
      "[INFO] processing image 115/404\n",
      "[INFO] processing image 116/404\n",
      "[INFO] processing image 117/404\n",
      "[INFO] processing image 118/404\n",
      "[INFO] processing image 119/404\n",
      "[INFO] processing image 120/404\n",
      "[INFO] processing image 121/404\n",
      "[INFO] processing image 122/404\n",
      "[INFO] processing image 123/404\n",
      "[INFO] processing image 124/404\n",
      "[INFO] processing image 125/404\n",
      "[INFO] processing image 126/404\n",
      "[INFO] processing image 127/404\n",
      "[INFO] processing image 128/404\n",
      "[INFO] processing image 129/404\n",
      "[INFO] processing image 130/404\n",
      "[INFO] processing image 131/404\n",
      "[INFO] processing image 132/404\n",
      "[INFO] processing image 133/404\n",
      "[INFO] processing image 134/404\n",
      "[INFO] processing image 135/404\n",
      "[INFO] processing image 136/404\n",
      "[INFO] processing image 137/404\n",
      "[INFO] processing image 138/404\n",
      "[INFO] processing image 139/404\n",
      "[INFO] processing image 140/404\n",
      "[INFO] processing image 141/404\n",
      "[INFO] processing image 142/404\n",
      "[INFO] processing image 143/404\n",
      "[INFO] processing image 144/404\n",
      "[INFO] processing image 145/404\n",
      "[INFO] processing image 146/404\n",
      "[INFO] processing image 147/404\n",
      "[INFO] processing image 148/404\n",
      "[INFO] processing image 149/404\n",
      "[INFO] processing image 150/404\n",
      "[INFO] processing image 151/404\n",
      "[INFO] processing image 152/404\n",
      "[INFO] processing image 153/404\n",
      "[INFO] processing image 154/404\n",
      "[INFO] processing image 155/404\n",
      "[INFO] processing image 156/404\n",
      "[INFO] processing image 157/404\n",
      "[INFO] processing image 158/404\n",
      "[INFO] processing image 159/404\n",
      "[INFO] processing image 160/404\n",
      "[INFO] processing image 161/404\n",
      "[INFO] processing image 162/404\n",
      "[INFO] processing image 163/404\n",
      "[INFO] processing image 164/404\n",
      "[INFO] processing image 165/404\n",
      "[INFO] processing image 166/404\n",
      "[INFO] processing image 167/404\n",
      "[INFO] processing image 168/404\n",
      "[INFO] processing image 169/404\n",
      "[INFO] processing image 170/404\n",
      "[INFO] processing image 171/404\n",
      "[INFO] processing image 172/404\n",
      "[INFO] processing image 173/404\n",
      "[INFO] processing image 174/404\n",
      "[INFO] processing image 175/404\n",
      "[INFO] processing image 176/404\n",
      "[INFO] processing image 177/404\n",
      "[INFO] processing image 178/404\n",
      "[INFO] processing image 179/404\n",
      "[INFO] processing image 180/404\n",
      "[INFO] processing image 181/404\n",
      "[INFO] processing image 182/404\n",
      "[INFO] processing image 183/404\n",
      "[INFO] processing image 184/404\n",
      "[INFO] processing image 185/404\n",
      "[INFO] processing image 186/404\n",
      "[INFO] processing image 187/404\n",
      "[INFO] processing image 188/404\n",
      "[INFO] processing image 189/404\n",
      "[INFO] processing image 190/404\n",
      "[INFO] processing image 191/404\n",
      "[INFO] processing image 192/404\n",
      "[INFO] processing image 193/404\n",
      "[INFO] processing image 194/404\n",
      "[INFO] processing image 195/404\n",
      "[INFO] processing image 196/404\n",
      "[INFO] processing image 197/404\n",
      "[INFO] processing image 198/404\n",
      "[INFO] processing image 199/404\n",
      "[INFO] processing image 200/404\n",
      "[INFO] processing image 201/404\n",
      "[INFO] processing image 202/404\n",
      "[INFO] processing image 203/404\n",
      "[INFO] processing image 204/404\n",
      "[INFO] processing image 205/404\n",
      "[INFO] processing image 206/404\n",
      "[INFO] processing image 207/404\n",
      "[INFO] processing image 208/404\n",
      "[INFO] processing image 209/404\n",
      "[INFO] processing image 210/404\n",
      "[INFO] processing image 211/404\n",
      "[INFO] processing image 212/404\n",
      "[INFO] processing image 213/404\n",
      "[INFO] processing image 214/404\n",
      "[INFO] processing image 215/404\n",
      "[INFO] processing image 216/404\n",
      "[INFO] processing image 217/404\n",
      "[INFO] processing image 218/404\n",
      "[INFO] processing image 219/404\n",
      "[INFO] processing image 220/404\n",
      "[INFO] processing image 221/404\n",
      "[INFO] processing image 222/404\n",
      "[INFO] processing image 223/404\n",
      "[INFO] processing image 224/404\n",
      "[INFO] processing image 225/404\n",
      "[INFO] processing image 226/404\n",
      "[INFO] processing image 227/404\n",
      "[INFO] processing image 228/404\n",
      "[INFO] processing image 229/404\n",
      "[INFO] processing image 230/404\n",
      "[INFO] processing image 231/404\n",
      "[INFO] processing image 232/404\n",
      "[INFO] processing image 233/404\n",
      "[INFO] processing image 234/404\n",
      "[INFO] processing image 235/404\n",
      "[INFO] processing image 236/404\n",
      "[INFO] processing image 237/404\n",
      "[INFO] processing image 238/404\n",
      "[INFO] processing image 239/404\n",
      "[INFO] processing image 240/404\n",
      "[INFO] processing image 241/404\n",
      "[INFO] processing image 242/404\n",
      "[INFO] processing image 243/404\n",
      "[INFO] processing image 244/404\n",
      "[INFO] processing image 245/404\n",
      "[INFO] processing image 246/404\n",
      "[INFO] processing image 247/404\n",
      "[INFO] processing image 248/404\n",
      "[INFO] processing image 249/404\n",
      "[INFO] processing image 250/404\n",
      "[INFO] processing image 251/404\n",
      "[INFO] processing image 252/404\n",
      "[INFO] processing image 253/404\n",
      "[INFO] processing image 254/404\n",
      "[INFO] processing image 255/404\n",
      "[INFO] processing image 256/404\n",
      "[INFO] processing image 257/404\n",
      "[INFO] processing image 258/404\n",
      "[INFO] processing image 259/404\n",
      "[INFO] processing image 260/404\n",
      "[INFO] processing image 261/404\n",
      "[INFO] processing image 262/404\n",
      "[INFO] processing image 263/404\n",
      "[INFO] processing image 264/404\n",
      "[INFO] processing image 265/404\n",
      "[INFO] processing image 266/404\n",
      "[INFO] processing image 267/404\n",
      "[INFO] processing image 268/404\n",
      "[INFO] processing image 269/404\n",
      "[INFO] processing image 270/404\n",
      "[INFO] processing image 271/404\n",
      "[INFO] processing image 272/404\n",
      "[INFO] processing image 273/404\n",
      "[INFO] processing image 274/404\n",
      "[INFO] processing image 275/404\n",
      "[INFO] processing image 276/404\n",
      "[INFO] processing image 277/404\n",
      "[INFO] processing image 278/404\n",
      "[INFO] processing image 279/404\n",
      "[INFO] processing image 280/404\n",
      "[INFO] processing image 281/404\n",
      "[INFO] processing image 282/404\n",
      "[INFO] processing image 283/404\n",
      "[INFO] processing image 284/404\n",
      "[INFO] processing image 285/404\n",
      "[INFO] processing image 286/404\n",
      "[INFO] processing image 287/404\n",
      "[INFO] processing image 288/404\n",
      "[INFO] processing image 289/404\n",
      "[INFO] processing image 290/404\n",
      "[INFO] processing image 291/404\n",
      "[INFO] processing image 292/404\n",
      "[INFO] processing image 293/404\n",
      "[INFO] processing image 294/404\n",
      "[INFO] processing image 295/404\n",
      "[INFO] processing image 296/404\n",
      "[INFO] processing image 297/404\n",
      "[INFO] processing image 298/404\n",
      "[INFO] processing image 299/404\n",
      "[INFO] processing image 300/404\n",
      "[INFO] processing image 301/404\n",
      "[INFO] processing image 302/404\n",
      "[INFO] processing image 303/404\n",
      "[INFO] processing image 304/404\n",
      "[INFO] processing image 305/404\n",
      "[INFO] processing image 306/404\n",
      "[INFO] processing image 307/404\n",
      "[INFO] processing image 308/404\n",
      "[INFO] processing image 309/404\n",
      "[INFO] processing image 310/404\n",
      "[INFO] processing image 311/404\n",
      "[INFO] processing image 312/404\n",
      "[INFO] processing image 313/404\n",
      "[INFO] processing image 314/404\n",
      "[INFO] processing image 315/404\n",
      "[INFO] processing image 316/404\n",
      "[INFO] processing image 317/404\n",
      "[INFO] processing image 318/404\n",
      "[INFO] processing image 319/404\n",
      "[INFO] processing image 320/404\n",
      "[INFO] processing image 321/404\n",
      "[INFO] processing image 322/404\n",
      "[INFO] processing image 323/404\n",
      "[INFO] processing image 324/404\n",
      "[INFO] processing image 325/404\n",
      "[INFO] processing image 326/404\n",
      "[INFO] processing image 327/404\n",
      "[INFO] processing image 328/404\n",
      "[INFO] processing image 329/404\n",
      "[INFO] processing image 330/404\n",
      "[INFO] processing image 331/404\n",
      "[INFO] processing image 332/404\n",
      "[INFO] processing image 333/404\n",
      "[INFO] processing image 334/404\n",
      "[INFO] processing image 335/404\n",
      "[INFO] processing image 336/404\n",
      "[INFO] processing image 337/404\n",
      "[INFO] processing image 338/404\n",
      "[INFO] processing image 339/404\n",
      "[INFO] processing image 340/404\n",
      "[INFO] processing image 341/404\n",
      "[INFO] processing image 342/404\n",
      "[INFO] processing image 343/404\n",
      "[INFO] processing image 344/404\n",
      "[INFO] processing image 345/404\n",
      "[INFO] processing image 346/404\n",
      "[INFO] processing image 347/404\n",
      "[INFO] processing image 348/404\n",
      "[INFO] processing image 349/404\n",
      "[INFO] processing image 350/404\n",
      "[INFO] processing image 351/404\n",
      "[INFO] processing image 352/404\n",
      "[INFO] processing image 353/404\n",
      "[INFO] processing image 354/404\n",
      "[INFO] processing image 355/404\n",
      "[INFO] processing image 356/404\n",
      "[INFO] processing image 357/404\n",
      "[INFO] processing image 358/404\n",
      "[INFO] processing image 359/404\n",
      "[INFO] processing image 360/404\n",
      "[INFO] processing image 361/404\n",
      "[INFO] processing image 362/404\n",
      "[INFO] processing image 363/404\n",
      "[INFO] processing image 364/404\n",
      "[INFO] processing image 365/404\n",
      "[INFO] processing image 366/404\n",
      "[INFO] processing image 367/404\n",
      "[INFO] processing image 368/404\n",
      "[INFO] processing image 369/404\n",
      "[INFO] processing image 370/404\n",
      "[INFO] processing image 371/404\n",
      "[INFO] processing image 372/404\n",
      "[INFO] processing image 373/404\n",
      "[INFO] processing image 374/404\n",
      "[INFO] processing image 375/404\n",
      "[INFO] processing image 376/404\n",
      "[INFO] processing image 377/404\n",
      "[INFO] processing image 378/404\n",
      "[INFO] processing image 379/404\n",
      "[INFO] processing image 380/404\n",
      "[INFO] processing image 381/404\n",
      "[INFO] processing image 382/404\n",
      "[INFO] processing image 383/404\n",
      "[INFO] processing image 384/404\n",
      "[INFO] processing image 385/404\n",
      "[INFO] processing image 386/404\n",
      "[INFO] processing image 387/404\n",
      "[INFO] processing image 388/404\n",
      "[INFO] processing image 389/404\n",
      "[INFO] processing image 390/404\n",
      "[INFO] processing image 391/404\n",
      "[INFO] processing image 392/404\n",
      "[INFO] processing image 393/404\n",
      "[INFO] processing image 394/404\n",
      "[INFO] processing image 395/404\n",
      "[INFO] processing image 396/404\n",
      "[INFO] processing image 397/404\n",
      "[INFO] processing image 398/404\n",
      "[INFO] processing image 399/404\n",
      "[INFO] processing image 400/404\n",
      "[INFO] processing image 401/404\n",
      "[INFO] processing image 402/404\n",
      "[INFO] processing image 403/404\n",
      "[INFO] processing image 404/404\n",
      "[INFO] serializing encodings...\n"
     ]
    }
   ],
   "source": [
    "calibration_process.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
